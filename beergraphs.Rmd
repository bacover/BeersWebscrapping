---
title: "Simple_Linear_Regression_Homework_Solutions"
author: "NYC Data Science Academy"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

## Question 1: Anatomical Data from Domestic Cats
**Purpose: Demonstrating understanding of how to run the models**

1. **Load: ** 
    + Load the *cats* from the **MASS** library. 
    + This dataset includes the body and heart weights of both female and male adult domestic cats. 
```{r}
library(MASS)
library(dplyr)
library(tidyr)
library(shiny)
library(data.table)
library(plotly)
library(usmap)
library(rsconnect)


beerdata <- read.csv(file="./beers/beers4.csv")
fctr.cols <- sapply(beerdata, is.factor)
beerdata[, fctr.cols] <- sapply(beerdata[, fctr.cols], as.character)
#beerdata = select(beerdata, Name=Name, Brewery=Brewery, Style=Style, ABV=ABV, Country=Country, State=State, Availability=Availability, Score=Score, Avg=Avg, Ratings=Ratings, Reviews=Reviews)
#beerdata = arrange(beerdata, desc(Score))

#summary(cats)
# create df of how many reviews each unique beer has gotten

# create df of mean and median for easy plot labeling
review_measures <- beerdata %>%
  summarise(Mean = round(mean(Reviews)),
            Median = median(Reviews))%>%
  # gather makes it a tidy df so I can easily get legend labels
  gather()
review_measures
rating_measures <- beerdata %>%
  summarise(Mean = round(mean(Ratings)),
            Median = median(Ratings))%>%
  # gather makes it a tidy df so I can easily get legend labels
  gather()
rating_measures

beerdata_review = select(beerdata, Name=Name, Reviews=Reviews)
beerdata_ratings = select(beerdata, Name=Name, Ratings=Ratings)

# create plot
beerdata_review %>%
  ggplot(aes(Reviews))+
  # I like this blue
  geom_histogram(fill = "#0072B2", color = 'white', alpha = .8)+
  # log scale to have an interpretable plot--most beers get very few reviews
#  scale_x_log10()+
  scale_x_continuous(limits = c(0,60))+
  # use review_measures to add mean/median lines to plot
  geom_vline(data = review_measures,
             aes(xintercept = value, color = key),
             lty = 2,
             size = .9)+
  # adjust y axis limits for 'better' looking graph
  scale_y_continuous(limits = c(0,50000))+
#  scale_y_continuous(limits = c(0,10000),
#                     expand = c(0,0))+
#                     labels = comma_format())+
  theme(legend.position = c(.75, .87))+
  labs(x = "# of Reviews",
       y = "# of Beers",
       title = "How many reviews do beers get?",
 #      subtitle = "~80% of beers receive 10 or fewer reviews",
       color = "")
#  my_theme_tweaks()

# create plot
beerdata_ratings %>%
  ggplot(aes(Ratings))+
  # I like this blue
  geom_histogram(fill = "#0072B2", color = 'white', alpha = .8)+
  # log scale to have an interpretable plot--most beers get very few reviews
#  scale_x_log10()+
  scale_x_continuous(limits = c(0,60))+
  # use review_measures to add mean/median lines to plot
  geom_vline(data = rating_measures,
             aes(xintercept = value, color = key),
             lty = 2,
             size = .9)+
  # adjust y axis limits for 'better' looking graph
  scale_y_continuous(limits = c(0,100000))+
  theme(legend.position = c(.75, .87))+
  labs(x = "# of Ratings",
       y = "# of Beers",
       title = "How many ratings do beers get?",
#       subtitle = "~80% of beers receive 10 or fewer ratings",
       color = "")
#  my_theme_tweaks()



```

2. **Visualize: **
    + Create a scatterplot of heart weight versus body weight. 
    + From this plot alone, do you think simple linear regression would be a good fit for the data? Explain.

```{r}

beerdata2 = filter(beerdata, Avg>0)
beerdata2 = filter(beerdata2, Reviews>10)
beergroup = group_by(beerdata2, Style)
#beergroup
beermeans = summarise(beergroup,Avg = median(Avg))
#beermeans
beermeans = arrange(beermeans, desc(-Avg))
#beermeans
#beermeans$Style
my_order = beermeans$Style
#my_order
top = top_n(beermeans, 15, Avg)
bottom = top_n(beermeans, -15, Avg)
mid_order = bind_rows(bottom, top)

#my_order
beerplot = ggplot(beergroup, aes(x=Style, y = Avg)) + geom_boxplot(coef=1.5) + scale_x_discrete(limits=mid_order$Style)+coord_flip()+geom_vline(xintercept = 16.5, linetype = 2, color = "#0072B2")+labs(x = "Beer Style",
       y = "Avg Score",
       title = "Top and Bottom Beer Styles",
#       subtitle = "~80% of beers receive 10 or fewer ratings",
       color = "")

beerplot

beerdata2 = filter(beerdata, !is.na(ABV))
beerdata2 = filter(beerdata2, Reviews>10)
beergroup = group_by(beerdata2, Style)
#beergroup
beermeans = summarise(beergroup,ABV = median(ABV))
#beermeans
beermeans = arrange(beermeans, desc(-ABV))
#beermeans
#beermeans$Style
my_order = beermeans$Style
#my_order
top = top_n(beermeans, 15, ABV)
bottom = top_n(beermeans, -11, ABV)
bottom
mid_order = bind_rows(bottom, top)

beerplot = ggplot(beergroup, aes(x=Style, y = ABV)) + geom_boxplot(coef=2) + scale_x_discrete(limits=mid_order$Style)+geom_vline(xintercept = 11.5, linetype = 2, color = "#0072B2")+coord_flip()+labs(x = "Beer Style",
       y = "ABV",
       title = "Top and Bottom ABV by Beer Style",
#       subtitle = "~80% of beers receive 10 or fewer ratings",
       color = "")
beerplot

```

3. **Model: **Regress heart weight onto body weight. For this model:
    + Write out the regression equation.
    + Interpret the meanings of the coefficients in context of the problem.
    + Are the coefficients significant? How can you tell?
    + Is the overall regression significant? How can you tell? How does your answer from Part C relate?
    + Find and interpret the RSE?
    + Find and interpret the coefficient of determination.
    
```{r}


beerdata_country = select(beerdata, Name=Name, Country=Country)
beerdata_country = filter(beerdata_country, Country != "United States")
beerdata_state = select(beerdata, Name=Name, State=State)
beerdata_state = filter(beerdata_state, State!="")
beerdata_state = arrange(beerdata_state, State)
beerdata_style = select(beerdata, Name=Name, Style=Style)

# create plot
beerdata_country %>%
  ggplot(aes(Country))+
  # I like this blue
  geom_histogram(fill = "#0072B2", color = 'white', alpha = .8, stat = "count")+coord_flip()+
  # log scale to have an interpretable plot--most beers get very few reviews
  # use review_measures to add mean/median lines to plot

  # adjust y axis limits for 'better' looking graph

  labs(x = "Country",
       y = "# of Beers",
       title = "Beers from Each Country, Minus United States",
#       subtitle = "~80% of beers receive 10 or fewer reviews",
       color = "")
#  my_theme_tweaks()

# create plot
beerdata_state %>%
  ggplot(aes(State))+
  # I like this blue
  geom_histogram(fill = "#0072B2", color = 'white', alpha = .8, stat = "count")+coord_flip()+
  # log scale to have an interpretable plot--most beers get very few reviews
  # use review_measures to add mean/median lines to plot

  # adjust y axis limits for 'better' looking graph

  labs(x = "State",
       y = "# of Beers",
       title = "Beers from Each State",
 #      subtitle = "~80% of beers receive 10 or fewer reviews",
       color = "")

# create plot
beerdata_style %>%
  ggplot(aes(Style))+
  # I like this blue
  geom_histogram(fill = "#0072B2", color = 'white', alpha = .8, stat = "count")+coord_flip()+
  # log scale to have an interpretable plot--most beers get very few reviews
  # use review_measures to add mean/median lines to plot

  # adjust y axis limits for 'better' looking graph

  labs(x = "Style",
       y = "# of Beers",
       title = "Beers from Each Style",
#       subtitle = "~80% of beers receive 10 or fewer reviews",
       color = "")
```

## Question 2: Machine Learning Theory
**Purpose: Demonstrate theory of lecture material**

1. **Assumptions: **
    + Assess each of the assumptions of linearity. 
    + Which assumption are you tring to correct if you decide to apply a Box-Cox transformation?

```{r}
plot(model)
#No overt deviations from the model assumptions are apparent; 
#there might be some non-constant variance as depicted by the 
#Scale-Location plot. This plot shows if residuals are spread 
#equally along the ranges of predictors. This is how you can 
#check the assumption of equal variance (homoscedasticity). 
#It's good if you see a horizontal line with equally (randomly) 
#spread points. http://data.library.virginia.edu/diagnostic-plots/
```


2. **Box-Cox Transformation: ** 
    + If you apply a Box-Cox transformation to a variable which you already applied a Box-Cox transformation, what would happen? 
    + If you are unsure, try doing it to see!

```{r}
boxcox(model.bc)  # this uses the box-cox model completed in the code below (Question 3)
#We can see that the lambda value of 1 is contained within the 
#95% confidence interval. This is an indication that a Box-Cox 
#transformation is probably not necessary at this point. 
#Because the variable has already been transformed (normalized), 
#it does not need to be again. 
```


3. **Evaluation: ** 
    + Why do we use the R^{2} coefficient of determination to validate a linear model rather than the RSS? 
    + Why does it make sense to square the residuals to help determine the best model?

```{text}
#The Rsquared value is standardized, a value closer to 1 is always 
#better, whereas RSS tells you the general size of the errors, and scales depending on your variable units and 
#the size of the dataset, which can be misleading. Squaring the residuals 
#in the equation causes the model to prefer a prediction that averages
#two observations with the same input value, rather than randomly choosing
#a value anywhere between the two observations' targets as the prediction. 
#Imagine there are 2 cats, each with a body weight of exactly 3 kg. One has 
#a heart weight of 10 g, while the other has a heart weight of 14 g. 
#If the predicted heart weight for that body weight ran anywhere between 
#10 and 14 g and the residuals were not squared, the error would just sum 
#to 4. (i.e., A prediction of 13g is 1g away from 14g, and 3g away from 10g, 
#1 + 3 = 4. The same would be true of any value between 10 and 14.) So the 
#model would not favor any one predicted value. With the residuals being 
#squared, however, a predicted heart weight of 13 g would have an error of
#10 (1^2 + 3^2), while a prediction of 12g would have an error of only 8 
#(2^2 + 2^2). Rsqaured is more affected by extreme outliers than an unsquared
#version would be. 
```


4. **Prediction and Confidence Bands: ** Why is the prediction band wider than the confidence band? Why does the confidence band widen as it travels away from the center of the regression line?

```{text}
#The confidence bands describe the range in which the regression line should lie
#The prediction bands describe the range in which the predicted values should lie.
#Since we expect residuals, the prediction band is wider than the confidence band.
#The confidence interval widens as it travels from the cetner of the regression line because
#the range of possible regression lines includes a range of possible slopes. 
#Since the line is centered at the horizontal center of the data, you can imagine 
#rotating the regression line slightly up and down about that center, thus changing 
#slope and creating the wideing toward the outside of the data.
```

## Question 3: Challenge Questions
**Purpose: Push yourself for more advanced topics**

These questions are an extension of the Question 1 coding questions from above.

1. **Visualize: **Add to your plot from Question 1 Part 2. 
    + Add the regression line.
    + Add the residuals. Do any of the residuals seem abnormally large?
    + Construct 95% confidence intervals for the model coefficients. Interpret the intervals in context of the problem.

```{r}
# plot the regression line
plot(cats$Bwt, 
     cats$Hwt, 
     xlab = "Body Weight (kg)", 
     ylab = "Heart Weight (g)",
     main = "Anatomical Data from Domestic Cats")
abline(model, lty = 2)

# plot the residuals
segments(cats$Bwt, 
         cats$Hwt,
         cats$Bwt, 
         (model$coefficients[1] + model$coefficients[2]*cats$Bwt),
         col = "red")
text(cats$Bwt - .03, 
     cats$Hwt, 
     round(model$residuals, 2), 
     cex = 0.5)

#The largest residual is from a cat with both the largest body and heart weights;
#its residual value is about 5.12. This doesn't appear to be too concerning at
#the moment, even though this value might be considered an outlier.

#-----------------with ggplot----------------------
ggplot(data = cats,aes(x = Bwt,y = Hwt)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red",se=F)+
  geom_segment(aes(x = Bwt,
                   xend = Bwt,
                   y = Hwt,
                   yend = (model$coefficients[1] + 
                             model$coefficients[2]*cats$Bwt)),
               color = "blue")
#---------------------------------------------------

#c confidence intervals
confint(model)

#We are 95% confident that the average heart weight of a cat with 
#no body mass is within (-1.73 g, 1.01 g).
#We are 95% confident that the average change in heart weight of 
#a cat as its body mass increases by 1 kg is within (3.54 g, 4.53 g).

newdata = data.frame(Bwt = seq(1.9, 4, length.out = 100))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")

plot(cats$Bwt, 
     cats$Hwt, 
     xlab = "Body Weight (kg)", 
     ylab = "Heart Weight (g)",
     main = "Anatomical Data from Domestic Cats")
abline(model, lty = 2)                              #Plotting the regression line.
lines(newdata$Bwt, conf.band[, 2], col = "blue")    #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue")    #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red")     #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red")     #Plotting the upper prediction band.
legend("topleft", 
       c("Reg. Line", "Conf. Band", "Pred. Band"),
       lty = c(2, 1, 1), 
       col = c("black", "blue", "red"))

#The confidence band is tighter than the prediction band because 
#it assesses the average heart weight for a given body weight; 
#the prediction band only assess a single instance of body weight.
#The certainty of the confidence band becomes lower as we travel 
#away from the heart of the data because there are fewer observations 
#upon which to base our local estimation.
```

2. **Apply: **Construct confidence and prediction intervals for body weights of 2.8 kg, 5 kg, and 10 kg. Do you foresee any issues with reporting any of these intervals?

```{r}
newdata = data.frame(Bwt = c(2.8, 5, 10))
predict(model, newdata, interval = "confidence")
predict(model, newdata, interval = "prediction")

#In general, predictions for observations outside the realm of our data are not
#good practice; it is difficult to extrapolate to a population on which we have
#no data. For example, here we attempt to generalize to cats with body weights
#of 10 kg, yet this is not nearly contained in the range of our dataset.
```

3. **Tune: **Transform your data using a Box-Cox transformation. 
    + Create a Box-Cox plot.
    + Choose the best value of lambda (Keep in mind interpretability).
    + Create a new regression and interpret your results.

```{r}
#boxcox plot
bc = boxcox(model)

#choose lambda
lambda = bc$x[which(bc$y == max(bc$y))]
lambda
#While the lambda value that maximizes the log-likelihood function
#is approximately 0.101, this is extremely close to 0 (which is 
#also contained within the 95% confidence interval). For the 
#purposes of balancing interpretation and accuracy, we will move 
#forward with a log transformation of our data.
Hwt.bc = log(cats$Hwt)
#Try to balance interpretability and accuracy; when taking this 
#perspective, there is not a completely correct answer

#new model
cats.bc = data.frame(Hwt = Hwt.bc, Bwt = cats$Bwt)
model.bc = lm(Hwt~Bwt, data = cats.bc)

#interpret results
summary(model.bc)
plot(model.bc)

#There doesn't appear to be any violated assumptions in the new model.
#Both coefficients in the model are significant.
```

4. **Visualize: **Plot the regression line from the Box-Cox model on the scatter plot of heart weight versus body weight.

```{r}
plot(cats$Bwt, 
     Hwt.bc, 
     xlab = "Body Weight (kg)",
     ylab = "Log Heart Weight (g)",
     main = "Anatomical Data from Domestic Cats\nBox-Cox Transformed")
abline(model.bc, lty = 2)

#--------------------ggplot to compare both models-----------------
pt = 2 + 1:200*0.01
pt = data.frame(Bwt=pt)
pred_bc = predict(model.bc, pt)
pred = predict(model, pt)

ggplot() + geom_point(data=cats, aes(Bwt, Hwt)) +
  geom_line(aes(pt, pred), color= 'blue')+
  geom_line(aes(pt, exp(pred_bc)), color='red')
#------------------------------------------------------------------
```

5. **Compare: **Compare the models you created:
    + Give one reason why you might use the original model instead of the Box-Cox transformed model.
    + Give one reason why you might use the Box-Cox transformed model instead of the original model.

```{text}
#One reason why we might use the original model over the Box-Cox transformed
#model is that it is more interpretable. Since we are not transforming any of
#the variables, the values stay on a natural scale that is easier to interpret.

#One reason why we might use the Box-Cox transformed model over the original model
#is that it seems to correct the minor potential assumption violations. The original
#model slightly deviated from the constant variance assumption and potentially the
#normality assumption, but the Box-Cox transformation attempts to fix these
#problems.
```
